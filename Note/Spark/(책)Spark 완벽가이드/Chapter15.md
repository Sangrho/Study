### CHAPTER 15 클러스터에서 스파크 실행하기   
기존에 공부했던 내용을 정리하자면,  
구조적 API 로 정의한 논리적인 연산을 논리적 실행 계획으로 분해한 다음 물리적 실행 계획으로 변환하는 과정으로 넘어간다.  
물리적 실행 계획은 클러스터의 머신에서 실행되는 단위인 RDD 작업으로 구성된다.  
  
#### 15.1 스파크 애플리케이션의 아키텍처   
- 드라이버  
스파크 애플리케이션의 '운전자 역할' 을 하는 프로세스.  
스파크 애플리케이션의 실행을 제어하고 스파크 클러스터의 모든 상태 정보를 유지한다.  
물리적 컴퓨팅 자원 확보와 익스큐터 실행을 위해 클러스터 매니저와 통신할 수 있어야 한다.  
  
- 익스큐터  
드라이버가 할당한 태스크를 수행하는 프로세스.  
드라이버가 할당한 태스크를 받아 실행하고 태스크의 상태와 결과를 드라이버에 보고한다.  
모든 스파크 애플리케이션은 개별 익스큐터 프로세스를 사용한다.  
  
- 클러스터 매니저  
스파크 애플리케이션을 실행할 클러스터 머신을 유지한다.  
아래 그림은 스파크 애플리케이션이 아직 실행되지 않았을 때의 상태이다.  
  
![image](https://user-images.githubusercontent.com/4033129/77845249-50ca2d80-71e8-11ea-831e-a180563c0128.png)  
  
스파크 애플리케이션을 실제로 실행할 때가 되면, 사용자 설정을 바탕으로 클러스터 매니저에 자원 할당을 요청한다.  
사용자 애플리케이션의 설정에 따라 스파크 드라이버를 실행할 자원을 포함해 요청하거나,스파크 애플리케이션 실행을 위한 익스큐터 자원을 요청할 수 있다.  
스파크 애플리케이션의 실행 과정에서 애플리케이션이 실행되는 머신을 관리한다.  
  
##### 15.1.1 실행모드  
애플리케이션을 실행할 때 요청한 자원의 물리적인 위치를 결정한다.  
- 클러스터 모드  
- 클라이언트 모드  
- 로컬 모드  
  
* 클러스터 모드  
1. 사용자가 만든 스파크 애플리케이션을 JAR 파일이나 스크립트를 클러스터 매니저에 전달한다.  
2. 클러스터 매니저는 파일을 받은 다음 워커 노드에 드라이버와 익스큐터 프로세스를 실행한다.  
3. 하나의 워커 노드에 드라이버를 할당하고 다른 워커 노드에 익스큐터를 할당한다.  
![image](https://user-images.githubusercontent.com/4033129/77845314-e796ea00-71e8-11ea-82cc-6bf5fa1f61e8.png)  
  
* 클라이언트 모드  
애플리케이션을 제출한 클라이언트 머신에 드라이버가 기동되는 것 외에는 클러스터 모드와 동일하다.  
관점을 달리 하면, 드라이버는 클러스터 외부의 머신에서 실행되며 나머지 워커는 클러스터에 위치하는 것을 알 수 있다.  
![image](https://user-images.githubusercontent.com/4033129/77845326-01d0c800-71e9-11ea-83e8-20e490670e86.png)  
  
* 로컬 모드  
모든 애플리케이션이 단일 머신에서 실행된다.  
  
#### 15.2 스파크 애플리케이션의 생애주기(스파크 외부)   
spark-submit 명령을 사용해 애플리케이션이 클러스터 모드에서 실행하는 과정을 알아본다.  
  
##### 15.2.1 클라이언트 요청  
스파크 애플리케이션이 컴파일된 JAR 나 스크립트를 제출하고, 이 시점에 로컬 머신에서 코드가 실행되어 클러스터 마스터 노드에 요청한다.  
이 과정에서 드라이버 프로세스의 자원을 함께 요청한다.  
클러스터 매니저는 이 요청을 받아들이고 클러스터 노드 중 하나에 드라이버 프로세스를 실행한다.  
스파크 잡을 제출한 클라이언트 프로세스는 종료되고 애플리케이션은 클러스터에서 실행된다.  
  
##### 15.2.2 시작  
사용자 코드에는 반드시 스파크 클러스터(드라이버/익스큐터) 를 초기화하는 SparkSession 이 포함되어야 한다.  
SparkSession 은 클러스터 매니저와 통신해서 스파크 익스큐터 프로세스의 실행을 요청한다.  
클러스터 매니저는 익스큐터 프로세스를 시작하고 결과를 응답받아 익스큐터의 위치와 관련 정보를 드라이버 프로세스로 전송한다.  
이 모든 과정이 정상적으로 완료되면 '스파크 클러스터' 가 완성이 되는 것이다.  
  
##### 15.2.3 실행  
클러스터 매니저에서 드라이버와 워커는 코드를 실행하고 데이터를 이동하는 과정에서 서로 통신한다.  
마스터는 각 워커에 태스크를 할당하고, 태스크를 할당받은 워커는 태스크의 상태와 성공/실패 여부를 드라이버에 전송한다.  
  
##### 15.2.4 완료   
스파크 애플리케이션의 실행이 완료되면, 스파크 드라이버 프로세스가 성공이나 실패 중 하나의 상태로 종료된다.  
클러스터 매니저는 드라이버가 속한 스파크 클러스터의 모든 익스큐터를 종료시킨다.  
이 시점에 스파크 애플리케이션의 성공/실패 여부를 클러스터 매니저에 요청해 확인할 수 있다.  
  
#### 15.3 스파크 애플리케이션의 생애주기(스파크 내부)   
##### 15.3.1 SparkSession  
대화형 모드가 아니고서는 직접 생성해야 한다.  
  
```  
from pyspark.sql import SparkSession  
spark = SparkSession.builder \  
	.master("yarn") \  
	.appName("Josh") \  
	.getOrCreate()  
```  
  
##### 15.3.2 논리적 명령  
  
```  
df1 = spark.range(2, 10000000, 2)  
df2 = spark.range(2, 10000000, 4)  
step1 = df1.repartition(5)  
step12 = df2.repartition(6)  
step2 = step1.selectExpr("id * 5 as id")  
step3 = step2.join(step12, ["id"])  
step4 = step3.selectExpr("sum(id)")  
  
step4.collect() # 결과는 2,500,000,000,000  
```  
위 코드에 대한 실행 계획정보는 아래와 같다.  
각 DataFrame, Project(Join 과 같은 연산)에 대한 id 가 지정된다.(6L, 2L)  
또한 자동으로 실행되는 hashpartitioninig 과 Sort 과정을 알 수 있다.  
join 은 기본적으로 inner Join 이라는 것도 알 수 있다.  
이 외에 자동으로 실행되는 부분은 무엇일까?  
  
  
```  
== Physical Plan ==  
*(7) HashAggregate(keys=[], functions=[sum(id#6L)])								# step4 = step3.selectExpr("sum(id)")  
+- Exchange SinglePartition														# [Desc.] 최종 스테이지에서 드라이버로 결과를 전송하기 전에 파티션마다 개별적으로 수행된 결과를 단일 파티션으로 모으는 작업을 수행  
   +- *(6) HashAggregate(keys=[], functions=[partial_sum(id#6L)])				  
      +- *(6) Project [id#6L]  
         +- *(6) SortMergeJoin [id#6L], [id#2L], Inner							# step3 = step2.join(step12, ["id"])  
            :- *(3) Sort [id#6L ASC NULLS FIRST], false, 0  
            :  +- Exchange hashpartitioning(id#6L, 200)							# [Desc.] spark.sqlshuffle.partitions 의 Default값인 200개의 셔플 파티션이 생성 되었다.  
            :     +- *(2) Project [(id#0L * 5) AS id#6L]						# step2 = step1.selectExpr("id * 5 as id")  
            :        +- Exchange RoundRobinPartitioning(5)						# step1 = df1.repartition(5)  
            :           +- *(1) Range (2, 10000000, step=2, splits=10)			# df1 = spark.range(2, 10000000, 2)  
            +- *(5) Sort [id#2L ASC NULLS FIRST], false, 0  
               +- Exchange hashpartitioning(id#2L, 200)  
                  +- Exchange RoundRobinPartitioning(6)							# step12 = df2.repartition(6)  
                     +- *(4) Range (2, 10000000, step=4, splits=10)				# df2 = spark.range(2, 10000000, 4)  
```  

collect 라는 액션을 통해 Stage/Task로 이루어진 Job이 실행되었다.  
  
* 논리적 명령을 물리적 실행 계획으로 변환하기  
  
##### 15.3.3 스파크 잡  
Job은 Stage 로 나뉘며, Stage 의 수는 셔플 작업이 얼마나 많이 발생하는지에 따라 달라진다.  
  
##### 15.3.4 스테이지  
- Stage 는 다수의 머신에서 동일한 연산을 수행하는 Task 의 그룹을 나타낸다.  
- 스파크는 가능한 한 많은 Task(Job 의 트랜스포메이션) 을 동일한 Stage 로 묶으려 노력한다.  
- 셔플 작업이 일어나면 반드시 다음에 새로운 Stage 를 시작하는데, 셔플은 데이터의 물리적 재분배 과정이기 때문이다. 파티션을 재분배하는 과정은 데이터를 이동시키는 작업이므로 익스큐터 간의 조정이 필요하다. 스파크는 셔플이 끝난 다음 새로운 스테이지를 시작하며 최종 결과를 계산하기 위해 스테이지 실행 순서를 계속 추적한다.  
  
<img width="1663" alt="image" src="https://user-images.githubusercontent.com/4033129/77846107-8d008c80-71ee-11ea-9931-ace7355645d0.png">  
  
spark.sql.shuffle.partitions 는 클러스터의 익스큐터 수보다 파티션 수를 더 크게 지정하는 것이 좋다.  
Stage 에서 처리해야 할 데이터양이 매우 많다면 클러스터의 CPU 코어당 최소 2~3개의 Task를 할당한다.  
파티션 수가 너무 적으면 소수의 노드만 작업을 수행하기 때문에 데이터 치우침 현상이 발생하며, 파티션 수가 너무 많으면 파티션을 처리하기 위한 Task 를 많이 실행해야 하므로 부하가 발생한다. 따라서 셔플을 수행할 때는 결과 파티션당 최소 수십 메가 바이트의 데이터가 포함되어 있어야 한다.  
  
##### 15.3.5 태스크  
Stage 는 Task 들로 구성된다.  
각 Task 는 단일 익스큐터에서 실행할 데이터의 블록과 다수의 트랜스포메이션 조합으로 볼 수 있다.  
만약 데이터셋이 거대한 하나의 파티션인 경우 하나의 Task 만 생성된다.  
만약 1,000개의 작은 파티션으로 구성되어 있따면 1,000 개의 Task 를 만들어 병렬로 실행할 수 있다.  
  
#### 15.4 세부 실행 과정   
1. map 연산 후 다른 map 연산이 이어진다면 함께 실행할 수 있도록 Stage 와 Task 를 자동으로 연결한다.  
2. 모든 셔플을 작업할 때 데이터를 안정적인 저장소(Disk) 에 저장하므로 여러 Job 에서 재사용할 수 있다.  
  
##### 15.4.1 파이프라이닝  
스파크를 '인메모리 컴퓨팅 도구' 로 만들어주는 핵심 요소 주 ㅇ하나는 메모리나 디스크에 데이터를 쓰기 전에 최대한 많은 단계를 수행 한다는 점이다.  
스파크가 수행하는 주요 최적화 기법 중 하나는 RDD 나 RDD 보다 더 아래에서 발생하는 파이프라이닝 기법이다.  
  
파이프라이닝 기법은,  
노드 간의 데이터 이동 없이 각 노드가 데이터를 직접 공급할 수 있는 연산만 모아 태스크의 단일 스테이지로 만든다.  
  
##### 15.4.2 셔플 결과 저장  
스파크가 reduce-by-key 연산 같이 노드 간 복제를 유발하는 연산을 실행하면,  
엔진에서 파이프라이닝을 수행하지 못하므로 네트워크 셔플이 발생한다.  
  
노드 간 복제를 유발하는 연산은 각 키에 대한 입력 데이터를 먼저 여러 노드로부터 복사한다. 항상 데이터 전송이 필요한 '소스' Task를 먼저 수행하기 때문이다.  
그리고 소스 Task 의 Stage 가 실행되는 동안 셔플 파일을 로컬 디스크에 기록한다.  
그런 다음 그룹화나 리듀스를 수행하는 Stage 가 시작된다.  
이 Stage 에서는 셔플 파일에서 레코드를 읽어 들인 다음 연산을 수행한다. 예를 들어 특정 범위의 키와 관련된 데이터를 읽고 처리한다. 만약 잡이 실패한 경우 셔플 파일을 디스크에 저장했기 때문에 '소스' Stage 가 아닌 해당 Stage 부터 처리할 수 있다. 따라서 '소스' Task 를 재실행할 필요 없이 실패한 리듀스 Task 부터 다시 시작할 수 있다.  
  
셔플 결과를 저장할 때 발생할 수 있는 부작용은 이미 셔플된 데이터를 이용해 새로운 Job을 실행하면 '소스'와 관련된 셔플이 다시 실행되지 않는다는 것이다.  
스파크는 다음 Stage 를 실행하는 과정에서 디스크에 이미 기록되어 있는 셔플 파일을 다시 사용할 수 있다고 판단하기 때문에 이전 Stage 를 처리하지 않는다.  
이 부분은 스파크 UI 와 로그 파일에서 'skipped' 라고 표시 된다.  
  
이러한 자동 최적화 기능은 동일한 데이터를 사용해 여러 잡을 실행하는 워크로드의 시간을 절약할 수 있다.  
더 나은 성능을 얻기 위해 cash메서드를 사용해서, 사용자가 직접 캐싱을 수행할 수 있어서 좋다.  
