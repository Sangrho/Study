### CHAPTER 26 분류   
분류는 주어진 입력 특징을 사용하여 레이블, 카테고리, 클래스 또는 이산형 변수를 예측하는 작업이다.  
회귀와 같은 다른 ML 작업과의 주요 차이점은 출력 레이블이 유한한 값의 집합이라는ㄴ 것이다.  
  
#### 26.1 활용 사례   
- 신용리스크 예측 :  금융회사는 회사나 개인에게 대출을 제공하기 전에 여러 가지 변수를 고려하며, 대출을 제공할지 여부는 이진 분류 문제이다.  
- 뉴스기사 분류   
- 사용자 행위 분류  
  
#### 26.2 분류 유형   
##### 26.2.1 이진 분류  
예측할 수 있는 레이블 수는 오직 두 개이며, 대표적인 사례로 Fraud Analytics 가 있다.  
  
##### 26.2.3 다중 클래스 분류  
3개 이상의 레이블 중에서 하나의 레이블이 선택되는 다중 클래스 분류가 있다.  
페이스북에서 주어진 사진의 사람을 맞히는 서비스, 기상청에서 날씨를 예측하는 예가 있다.  
예측 대상은 무한한 클래스가 아니라 항상 유한한 클래스 집합이다.  
  
##### 26.2.3 다중 레이블 분류  
주어진 입력에 대해 여러 레이블을 생성할 수 있다.  
책의 내용을 기반으로 그 책의 장르가 무엇인지 예측하는 예가 있다.  
  
#### 26.3 MLlib의 분류 모델   
* 스파크에서 이진 분류, 다중 클래스 분류를 지원하는 모델은 아래와 같다.  
	- 로지스틱 회귀  
	- 의사결정트리  
	- 랜덤 포레스트  
	- 그래디언트 부스티드 트리  
	- (다중 레이블 예측 지원 안 함)  
  
##### 26.3.1 모델 확장성  
  
![image](https://user-images.githubusercontent.com/4033129/83971879-68284400-a918-11ea-9b58-2decdfdc32b9.png)  
  
학습에 사용되는 데이터 크기에 제한이 없는 이유는 *확률적 경사 하강법* 이나 *L-BFGS* 와 같은 방법을 사용하여 학습하기 때문이다.  
  
#### 26.4 로지스틱 회귀   
가장 널리 사용되는 분류 기법의 하나로 개별 입력과 특정 가중치를 결합하여 특정 클래스테 속할 확률을 얻는 선형 방법론이다.  
가중치는 특징의 중요성을 잘 나타내기 때문에 유용하게 사용된다.  
예를 들어 어떤 특징에 높은 가중치가 부여됐을 때, 해다 ㅇ특징 내 값 간의 분산이 결과에 유의미한 영향을 미친다고 가정할 수 있다.  
  
##### 26.4.1 모델 하이퍼파라미터  
모델 하이퍼파라미터는 모델 자체의 기본 구조를 결정하는 환경 설정 항목이다.  
종류는 아래와 같다.  
	- family : 해당 모델의 클래스가 다항인지 이항인지 지정  
	- elasticNetParam : 0에서 1 사이의 부동소수점수를 지정한다.  
	- fitIntercept : true, false 값을 지정한다.  
	- regParam : 0보다 크거나 같은 값을 지정한다. 목적 함수에서 일반화 항에 얼마만큼의 가중치를 주는지 결정한다.  
	- standardization : 입력용 모델로 전달하기 전에 입력을 표준화할지 여부를 결정하기 위한 true 또는 false 값을 지정한다.  
  
##### 26.4.2 학습 파라미터  
학습 파라미터는 모델 학습을 수행하는 방법을 지정하는 데 사용된다.  
종류는 아래와 같다.  
	- maxIter : 총 학습 반복 횟수이다.변경해도 결과가 크게 달라지지는 않으므로 가장 먼저 조정할 파라미터는 아니다.  
	- tol : 파라미터의 변경으로 인해 가중치가 충분히 최적화되었음을 나타내는 입곗값을 지정하고 반복을 중지시킬 수 있다. 이 역시 가장 먼저 조정할 파라미터는 아니다.  
	- weitghtCol : 특정 로우에 가중치르 ㄹ부여하는 데 사용하는 가중치 컬럼의 이름이다. 특정 학습 예제가 얼마나 중요한지 나타내느 ㄴ척도가 있고 그와 관련한 가중치를 가지고 있는 경우에 유용하게 사용할 수 있다.  
  
##### 26.4.3 예측 파라미터  
예측 파라미터는 모델이 예측할 때 예측을 수행하는 방법을 결정하는 데는 영향을 주지만 학습에는 영향을 주지 않는다.  
종류는 아래와 같다.  
	- threshold : 0에서 1 범위의 Double 타입을 가진다. 주어진 클래스를 예측하는 확률 입곗값이다. 요구사항에 따라 조절하여 거짓양성과 거짓음성의 균형을 맞출 수 있다.  
	- thresholds : 다중 분류를 할 때 각 클래스에 대한 입곗값 배열을 지정할 수 있다.  
  
##### 26.4.4 실습 예제  
##### 26.4.5 모델 요약  
  
#### 26.5 의사결정트리   
분류 문제를 해결하는 데 가장 친근하고 해석 가능한 모델 중 하나이다.  
인간이 자주 활용하는 단순한 의사결정 모형과 매우 유사하기 때문이다.  
예는, 아이스크림을 프로모션하는 상황에서 어떤 사람이 구매할 것인지 여부를 예측해야 한다고 했을 때, 활용할 수 있는 유의미한 특징은 바로 개인별 아이스크림 선호 여부이다. 모든 입력을 사용하여 아이스크림을 구매할지 말지와 같은 구조를 만들어놓고, 예측할 시점에 해당 가지치기(branch)를 따른다.  
  
초기 모델링을 위한 좋은 방법이기는 하나 계산 시간이 많이 소요된다는 단점이 있다. 또한 데이터에 대해 극단적으로 과적합 될 수 있다.  
  
##### 26.5.1 모델 하이퍼파라미터  
종류는 아래와 같다  
	- maxDepth : 의사결정트리는 말 그대로 트리를 학습시키는 것이므로, 데이터셋에 과적합되는것을 방지하기 위해 최대 깊이를 지정하는 것이다.  
	- maxBins : 의사결정트리는 연속형ㅇㅇ 특징을 범주형 특징으로 변환한다. 연속형 특징을 범주형 특징으로 변환하는 경우 일반적으로 구간화(binning) 을 수행하는데, maxBins 는 연속형 특징을 대상으로 구간화를 할 때 생성해야 하는 구간 수를 결정한다. 생성되는 구간이 많을수록 좀 더 세분화된 분석을 할 수 있다.  
	- impurity : 트리를 구성하려면 모델이 가지치기를 해야 하는 조건을 정의해야 한다. impurity는 모델이 특정 말단 노드에서 분할되어야 하는지 여부를 결정하기 위한 지표를 의미한다. 이 파라미터에서는 일반적으로 사용되는 두 가지 불순도 측정 기준인 엔트로비나 지니 중 하나를 설정할 수 있다.  
	- minInfoGain : 의사결정트리의 분할에 사용할 수 있는 최소 정보 획득을 결정한다. 값이 클수록 과적합을 방지할 수 있다.  
	- minInstancePerNode : 모델이 학습해야 할 최소 인스턴스 수를 지정하여 특정 노드에서 의사결정트리의 생성을 종료하도록 한다. 이 파라미터를 이용하여 의사결정트리의 깊이를 제한하거나 어떤 말단 노드에서 멈춰야 하는지 결정하는 최소 학습 인스턴스 수를 지정하여 모델의 과적합을 방지할 수 있다.  
  
##### 학습 파라미터  
종류는 아래와 같다.  
	-checkpointInterval : 체크포인팅은 학습 과정 동안 진행되는 모델의 작업 내용을 저장하는 방법으로 해당 파라미터를 지정하면 클러스터 내 특정 노드가 다양한 이유로 충돌할 경우에도 이전까지 진행된 작업 내용을 복구할 수 있다.  
  
##### 예측 파라미터		  
  
#### 26.6 랜덤 포레스트와 그래디언트 부스티드 트리   
랜덤 포레스트와 그래디언트 부스티드 트리는 의사결정트리의 확장된 알고리즘이다.  
일반적인 의사결정트리 알고리즘처럼 전체 데이터셋으로 하나의 트리를 생성하는 것이 아니라 다양한 서브 데이터셋으로 여러 개의 트리를 생성한다.  
이는 각 트리가 특정 도메인 데이털르 학습할수록 '전문가(의사결정 트리 모델' 가 되는 특성을 응용한 것으로 일부 의사결정트리의 특정 영역을 학습하는 동안 다른 의사결정트리는 다른 영역을 분담하여 학습하는 개념이다.  
즉, 다양한 도메인(데이터)를 학습한 전문가를 결합하여 그룹의 성과가 어떤 개인보다도 높은 '집단지성' 효과를 얻는 원리이며, 이러한 방법론을 사용함녀 의사결정트리 알고리즘의 과적합 이슈를 방지하는 데도 도움이 될 수 있다.  
  
둘의 차이는 아래와 같다.  
	- 랜덤 포레스트 : 단순히 많은 트리를 학습시킨 다음 각 트리의 응답을 평균화하여 최종 예측을 한다.  
	- 그래디언트 부스티드 트리에서도 마찬가지로 많은 트리를 학습시키지만 랜덤 포레스트와는 다르게 개별 트리를 학습할 때 별도의 가중치가 부여된다. 또한 각 트리는 대체로 동일한 파라미터를 가지고 있다.  
  
##### 26.6.1 모델 하이퍼파라미터  
* 랜덤 포레스트에서만 사용되는 하이퍼파라미터  
	- numTrees : 학습시킬 트리 수  
	- featureSubsetStrategy : 분할을 할 때 고려해야 할 특징 수를 결정한다.  
* 그래디언트 부스티드 트리에서만 사용되는 하이퍼파라미  
	- lossType : 학습하는 과정에서 손실을 최소화하기 위한 손실 함수를 지정하는 하이퍼파라미  
	- maxIter : 총 학습 반복 횟  
	- stepSize : 알고리즘의 학습  
	  
##### 26.6.2 학습 파라미터  
checkpointInterval 만을 제공한다.  
  
##### 26.6.3 예측 파라미터  
의사결정트리와 동일한 예측 파라미터를 가진다.  
  
#### 26.7 나이브 베이즈   
베이즈의 정리에 기반한 분류기의 모음이다.  
이 모델의 핵심 가정은 데이터의 모든 특징이 서로 독립적이라는 것이다.  
물로 ㄴ이론적으로 완벽한 독립이 아닐지라도 ㅊ우분히 유용한 모델을 생성할 수 있다.  
일반적으로 텍스트나 문서를 분류하는 문제에서 주로 사용되지만 좀 더 범용적인 분류기로도 사용될 수 있다.  
두 가지 모델 유형이 있다.  
  
1. 다변량 베르누이 모델 : 문서 내 용어의 존재를 지시변수를 활용하여 나타낸다.  
2. 다항 모델 : 용어의 총 수를 변수로 활  
  
중요한 점은 모든 입력 특징이 음수가 아니어야 한다는 것이다.  
  
##### 26.7.1 모델 하이퍼파라미터  
모델의 기본 구조를 결정하기위한 종류는 아래와 같다.  
	- modelType : bemoulli, multinormial  
	- weightCol : 각 데이터값에 대해 서로 다른 가중치를 줄 수 있다.  
  
##### 26.7.2 학습 파라미터  
학습 수행 방식을 지정하는 종류는 아래와 같다.  
	- smoothing : 가산적 평활화를 사용하여 일반화 작업의 범위를 결정한다. 이렇게 하면 범주 데이터를 평활하하고 특정 클래스의 예상 확률을 변경함으로써 학습 데이터에 대한 과적합을 피할 수 있다.  
  
##### 26.7.3 예측 파라미터  
다른 모델과 동일한 threasholds 이다	  
  
#### 26.8 분류와 자동 모델 튜닝을 위한 평가기   
평가기는 모델에 대한 성공의 척도를 지정할 수 있다.  
평가기는 독립적으로 사용할 때는 큰 도움이 되지 않지만, 파이프라인에서 사용하면 모델 및 변환자의 다양한 파라미터에 대한 그리드 서치를 자동화할 수 있다.  
즉, 파라미터 간 모든 조합을 시도하여 어떤 파라미터 조합이 가장 우수한 성능을 보이는지 확인할 수 있다.  
  
평가기는 이 파이프라인과 그리드 컨텍스트 파라미터에서 가장 유용하다. 분류 문제에는 두 개의 평가기가 있으며 두 개 컬럼으로 존재한다.   
  
#### 26.9 세부 평가지표   
  
#### 26.10 일대다 분류기   
  
#### 26.11 다층 퍼셉트론   
  
#### 26.12 정리   
