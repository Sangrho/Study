## 장애포인트
Druid 는 내부에 많은 어플리케이션이 있으며 외부로도 많은 Dependency가 있다.
따라서 서로 간에 영향도를 미리 파악하고, 장애 발생시 맥을 짚어야 하는 포인트를 알아야만 한다.

## 장애포인트 세부

[ Application Layer ]

1.Zookeeper<br/>
죽기전 알고 있던 클러스터와 통신하기 때문에,쿼리 수행이 가능하다.<br/>
왜냐하면Broker가 Zookeeper가 죽기 전후의 클러스터는 바뀌지 않을거라 생각 하기 때문이다.<br/><br/>

2.Coordinator<br/>
Segment Granularity 가1hour로 되어있다는 가정하에,현재 1시간동안은 문제 없이 수행되지만,다음 시간부터 균등하게 적재가 안된다.<br/>
왜냐하면 어느 노드에 얼만큼의 데이터가 축적되서,어디에 적재 해야 하는지에 대한 가이드를 못 해주기 때문이다.<br/><br/>

3.META_DATA<br/>
이게 죽으면 엄청 치명적이다.그래서 AIH RT처럼 3중화(Percona)해놔야 한다.<br/>
오퍼레이션 정보를 MySQL에 저장한다.따라서 MySQL이 죽으면,새로운 세그먼트의 생성,오래된 세그먼트의 드랍등을 할 수 없게 된다.<br/>
나머지Broker, Historical등은 정상적으로 동작하지만,세그먼트의 업데이트는 이뤄지지 않는다.<br/>
20170706에 있었던 일인데,mysql이 test용으로 총 두개가 떠있고,coo,zoo가 바라보는거와 middle이 바라보는게 다를 경우 log로 남지도 않고 경험과 감으로 때려맞춰야만 한다.<br/><br/>

4.HDFS<br/>
이게 죽으면 Peon은 더이상 데이터를 Memory에 로드하지 못하며,Peon도 더이상 값을 넣지 못하기 때문에(Persist)Pending상태가 된다.또한 기존 데이터들은 모두 날라가게 된다.<br/>
replication 1일 경우,historical마저 죽으면 데이터가 다 날라간다.<br/><br/>

5.Historical Node<br/>
AIH에서도 있었던 일인데,해당 노드의 디스크가 꽉 찰 경우,꽉 찬 이후부터의 데이터에 대한 Query응답을 할 수 없게 된다.<br/>
안에 있던 세그먼트들은 다시 재 할당(수동)되어야 한다.대부분 캐파의 문제이므로 캐파를 잘 따져본다.<br/><br/>

[ Client Layer ]

1. 데이터 처리 Task 가 생성되지 않는 경우<br/>

- Overlord 와 Middlemanager 가 정상적으로 실행되는지 확인<br/>
- Tranquility 실행 유무와 에러로그가 있는지 확인<br/>
- Tranquility 의 Spec 에서 windowPeriod 설정값이 너무 큰 값이거나 segmentGranularity 설정값보다 크게 되어 있는지 확인<br/><br/>

2.Task 가 종료되지 않거나 데이터 로딩이 안되는 경우<br/>

- 메타 DB 가 정상 동작중인지 확인<br/>
- Overlord 와 메타 DB 가 정상적으로 통신되는지 확인(DB 접속 계정 정보 등..)<br/>
- Coordinator, Historical 정상적으로 실행되는지 확인<br/>
- Historical 설정 중 Persist Storage(HDFS 등..) 설정이 정상적인지 확인<br/><br/>

3.Historical 에 Segment 가 로딩되지 않는 경우<br/>

- Segment 를 메모리에 로딩하기 위한 여유 메모리가 있는지 확인<br/>
- Historical 설정 중 Persiste Storage(HDFS 등..) 설정이 정상적인지 확인<br/>
- Historical Disk 공간이 부족한지 확인<br/><br/>

4.Druid 쿼리가 실패하거나 결과가 안나오는 경우<br/>

- Broker 상적으로 실행되는지 확인<br/>
- Coordinator Web 을 통해 쿼리 조회 범위에 해당되는 Segment 가 정상적으로 로드되어 있는지 확인<br/>
- 쿼리문법 확인 <br/>


